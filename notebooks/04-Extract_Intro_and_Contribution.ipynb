{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles processed: 973\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file containing the search results\n",
    "# with open(\"../data/02_document_search_results.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = json.load(file)\n",
    "data = pd.read_json(\"../data/02_document_search_results.json\")\n",
    "\n",
    "# Print count for verification\n",
    "print(f\"Total articles processed: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Patterns \n",
    "Define patterns to differentiate from the introduction and contribution part of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expanded regex patterns to match descriptive sentences\n",
    "patterns = [\n",
    "    r\"\\b[Tt]his (article|work|study|paper|research|review|survey|chapter|viewpoint)\\b\",\n",
    "    r\"\\b[Ii]n this (work|study|paper|research|review|survey|chapter|viewpoint)\\b\",\n",
    "    r\"\\b[Ww]e (propose|introduce|present|develop|describe|demonstrate|report|discuss|analyze|examine|investigate|explore|evaluate|address|outline)\\b\",\n",
    "    r\"\\b[Ii]n this (manuscript|article|contribution|viewpoint|approach|framework|investigation|analysis|implementation)\\b\",\n",
    "    r\"\\b[Tt]he (article|paper|study|work|research|review|survey|manuscript|current study|present study|present work|current work)\\b\",\n",
    "    r\"\\b[Oo]ur (work|study|paper|chapter|viewpoint|research|approach|framework|method|system|contribution|focus|aim|objective|goal)\\b\",\n",
    "    r\"\\b[Tt]his (manuscript|contribution|investigation|viewpoint|analysis|implementation|approach|framework|method|system)\\b\",\n",
    "    r\"\\b[Tt]he (purpose|aim|goal|objective) of this (paper|work|study|research|article|chapter|manuscript)\\b\",\n",
    "    r\"\\b[Hh]ere(,)? we\\b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Introduction and Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_introduction_text(text, patterns):\n",
    "    \"\"\"\n",
    "    Extracts text from the beginning of the abstract until it reaches a sentence\n",
    "    that matches one of the specified patterns (typically where authors start\n",
    "    describing their specific work).\n",
    "\n",
    "    If no pattern is found, returns the entire text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        patterns (list): List of regex patterns to match\n",
    "\n",
    "    Returns:\n",
    "        str: Text until the first pattern match, or the entire text if no pattern is found\n",
    "    \"\"\"\n",
    "    # Handle empty text\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "\n",
    "    # Collect sentences until a pattern match is found\n",
    "    result_sentences = []\n",
    "    pattern_found = False\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if sentence matches any pattern\n",
    "        if any(re.search(pattern, sentence) for pattern in patterns):\n",
    "            # Only consider it a match if we're not on the first sentence\n",
    "            if i > 0:\n",
    "                pattern_found = True\n",
    "                break\n",
    "\n",
    "        result_sentences.append(sentence)\n",
    "\n",
    "    # If no pattern was found, return the original text\n",
    "    if not pattern_found or len(result_sentences) < 1 or len(\" \".join(result_sentences)) < 50:\n",
    "        return text\n",
    "\n",
    "    # Join the collected sentences back together with spaces\n",
    "    result_text = \" \".join(result_sentences)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_contribution(text, patterns):\n",
    "    \"\"\"\n",
    "    Extracts text from the first sentence matching any of the specified patterns\n",
    "    until the end of the text (typically where authors start describing their specific work).\n",
    "\n",
    "    If no pattern is found, returns an empty string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        patterns (list): List of regex patterns to match\n",
    "\n",
    "    Returns:\n",
    "        str: Text from the first pattern match to the end of the text, or empty string if no pattern is found\n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "\n",
    "    # Find the index of the first sentence matching any pattern\n",
    "    start_idx = -1\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if any(re.search(pattern, sentence) for pattern in patterns):\n",
    "            start_idx = i\n",
    "            break\n",
    "\n",
    "    # If no pattern was found, return the input text\n",
    "    if start_idx == -1:\n",
    "        return text\n",
    "\n",
    "    # Join the sentences from the matching sentence until the end\n",
    "    result_text = \" \".join(sentences[start_idx:])\n",
    "    return result_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts processed: 973\n",
      "Number of abstracts missing or empty: 2\n",
      "Number of abstracts with introductions extracted: 971\n",
      "\n",
      "Number of abstracts where no pattern was found: 204\n",
      "Percentage with no pattern found: 26.53%\n",
      "Percentage with pattern found: 79.03%\n"
     ]
    }
   ],
   "source": [
    "# Add introduction text to the cleaned_data dictionary\n",
    "for i, item in enumerate(data.to_dict('records')):\n",
    "    # Check if abstract exists before processing and is not NaN\n",
    "    if \"abstract\" in item and item[\"abstract\"] and not pd.isna(item[\"abstract\"]):\n",
    "        item[\"introduction\"] = extract_introduction_text(item[\"abstract\"], patterns)\n",
    "    else:\n",
    "        # Handle missing or NaN abstracts\n",
    "        item[\"introduction\"] = \"\"\n",
    "    \n",
    "    # Update the DataFrame with the modified record\n",
    "    data.loc[i, \"introduction\"] = item[\"introduction\"]\n",
    "\n",
    "# Count abstracts with non-empty introductions\n",
    "abstracts_with_intros = [item for item in data.to_dict('records') if isinstance(item.get(\"introduction\"), str) and item.get(\"introduction\", \"\").strip()]\n",
    "\n",
    "# Count how many abstracts are missing\n",
    "abstracts_missing = len(data[data[\"abstract\"].isna() | (data[\"abstract\"] == \"\")])\n",
    "\n",
    "# Count abstracts where no pattern was found (full text was returned as introduction)\n",
    "full_text_abstracts = data[\n",
    "    data[\"abstract\"].notna() & \n",
    "    (data[\"abstract\"] != \"\") & \n",
    "    (data[\"introduction\"] == data[\"abstract\"])\n",
    "].to_dict('records')\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Number of abstracts processed: {len(data)}\")\n",
    "print(f\"Number of abstracts missing or empty: {abstracts_missing}\")\n",
    "print(f\"Number of abstracts with introductions extracted: {len(abstracts_with_intros)}\")\n",
    "\n",
    "# Statistics about abstracts where no pattern was found\n",
    "print(f\"\\nNumber of abstracts where no pattern was found: {len(full_text_abstracts)}\")\n",
    "if len(data) > 0:\n",
    "    print(\n",
    "        f\"Percentage with no pattern found: {len(full_text_abstracts) / (len(data) - len(full_text_abstracts)) * 100:.2f}%\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Percentage with pattern found: {(len(data) - len(full_text_abstracts)) / len(data) * 100:.2f}%\"\n",
    "    )\n",
    "else:\n",
    "    print(\"Percentage with no pattern found: N/A (no valid abstracts)\")\n",
    "\n",
    "# Display the first few introductory texts (up to 500 chars) along with their DOIs\n",
    "# for i, item in enumerate(abstracts_with_intros[:3]):\n",
    "#     print(f\"\\nIntroduction {i+1} (DOI: {item.get('doi', 'No DOI')}):\")\n",
    "#     intro = item[\"introduction\"]\n",
    "#     print(f\"{intro[:500]}...\" if len(intro) > 500 else intro)\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts processed: 973\n",
      "Number of abstracts with contributions extracted: 971\n",
      "Percentage with contributions extracted: 100.00%\n",
      "Average length of contributions: 1096.36 characters\n"
     ]
    }
   ],
   "source": [
    "# Add contribution text to the DataFrame\n",
    "for i, item in enumerate(data.to_dict('records')):\n",
    "    # Check if abstract exists before processing and is not NaN\n",
    "    if \"abstract\" in item and item[\"abstract\"] and not pd.isna(item[\"abstract\"]):\n",
    "        item[\"contribution\"] = extract_contribution(item[\"abstract\"], patterns)\n",
    "    else:\n",
    "        # Handle missing or NaN abstracts\n",
    "        item[\"contribution\"] = \"\"\n",
    "    \n",
    "    # Update the DataFrame with the modified record\n",
    "    data.loc[i, \"contribution\"] = item[\"contribution\"]\n",
    "\n",
    "# Count abstracts with non-empty contributions\n",
    "abstracts_with_contributions = [\n",
    "    item for item in data.to_dict('records') if item.get(\"contribution\", \"\").strip()\n",
    "]\n",
    "\n",
    "# Display statistics about extracted text segments\n",
    "print(f\"Number of abstracts processed: {len(data)}\")\n",
    "print(\n",
    "    f\"Number of abstracts with contributions extracted: {len(abstracts_with_contributions)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage with contributions extracted: {len(abstracts_with_contributions) / (len(data) - abstracts_missing) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Calculate average length only if there are contributions\n",
    "if abstracts_with_contributions:\n",
    "    average_length = sum(len(item['contribution']) for item in abstracts_with_contributions) / len(abstracts_with_contributions)\n",
    "    print(f\"Average length of contributions: {average_length:.2f} characters\")\n",
    "else:\n",
    "    print(\"Average length of contributions: N/A (no contributions found)\")\n",
    "\n",
    "# Display the first few contribution texts (up to 500 chars) along with their DOIs\n",
    "# for i, item in enumerate(abstracts_with_contributions[:3]):\n",
    "#     print(f\"\\nArticle {i+1} (DOI: {item.get('doi', 'No DOI')}):\")\n",
    "#     contribution = item[\"contribution\"]\n",
    "#     print(f\"{contribution[:500]}...\" if len(contribution) > 500 else contribution)\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean records without introduction or contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with introductions or contributions: 971\n"
     ]
    }
   ],
   "source": [
    "# Clean records without introductions or contributions\n",
    "cleaned_data = data[(data[\"introduction\"].str.strip() != \"\") | (data[\"contribution\"].str.strip() != \"\")]\n",
    "print(f\"Number of records with introductions or contributions: {len(cleaned_data)}\")\n",
    "\n",
    "# For backward compatibility, create a list of dictionaries if needed\n",
    "cleaned_data_records = cleaned_data.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a JSON file using pandas\n",
    "cleaned_data.to_json(\"../data/04_document_search_results_with_intros_and_contributions.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the contributions in a json format\n",
    "with open(\"../data/processed/04_document_search_results_contributions.json\", \"w\") as file:\n",
    "    json.dump(contribution_texts, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
