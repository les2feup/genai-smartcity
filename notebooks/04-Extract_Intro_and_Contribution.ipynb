{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles processed: 512\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file containing the search results\n",
    "with open(\"../data/02_document_search_results.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Print count for verification\n",
    "print(f\"Total articles processed: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expanded regex patterns to match descriptive sentences\n",
    "patterns = [\n",
    "    r\"\\b[Tt]his (article|work|study|paper|research|review|survey|chapter)\\b\",\n",
    "    r\"\\b[Ii]n this (work|study|paper|research|review|survey|chapter)\\b\",\n",
    "    r\"\\b[Ww]e (propose|introduce|present|develop|describe|demonstrate|report|discuss|analyze|examine|investigate|explore|evaluate|address|outline)\\b\",\n",
    "    r\"\\b[Ii]n this (manuscript|article|contribution|approach|framework|investigation|analysis|implementation)\\b\",\n",
    "    r\"\\b[Tt]he (article|paper|study|work|research|review|survey|manuscript|current study|present study|present work|current work)\\b\",\n",
    "    r\"\\b[Oo]ur (work|study|paper|chapter|research|approach|framework|method|system|contribution|focus|aim|objective|goal)\\b\",\n",
    "    r\"\\b[Tt]his (manuscript|contribution|investigation|analysis|implementation|approach|framework|method|system)\\b\",\n",
    "    r\"\\b[Tt]he (purpose|aim|goal|objective) of this (paper|work|study|research|article|chapter|manuscript)\\b\",\n",
    "    r\"\\b[Hh]ere(,)? we\\b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_introduction_text(text, patterns):\n",
    "    \"\"\"\n",
    "    Extracts text from the beginning of the abstract until it reaches a sentence\n",
    "    that matches one of the specified patterns (typically where authors start\n",
    "    describing their specific work).\n",
    "\n",
    "    If no pattern is found, returns the entire text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        patterns (list): List of regex patterns to match\n",
    "\n",
    "    Returns:\n",
    "        str: Text until the first pattern match, or the entire text if no pattern is found\n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "\n",
    "    # Collect sentences until a pattern match is found\n",
    "    result_sentences = []\n",
    "    pattern_found = False\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if sentence matches any pattern\n",
    "        if any(re.search(pattern, sentence) for pattern in patterns):\n",
    "            pattern_found = True\n",
    "            break\n",
    "\n",
    "        result_sentences.append(sentence)\n",
    "\n",
    "    # If no pattern was found, return the original text\n",
    "    if not pattern_found:\n",
    "        return text\n",
    "\n",
    "    # Join the collected sentences back together with spaces\n",
    "    result_text = \" \".join(result_sentences)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pattern_to_end(text, patterns):\n",
    "    \"\"\"\n",
    "    Extracts text from the first sentence matching any of the specified patterns\n",
    "    until the end of the text (typically where authors start describing their specific work).\n",
    "\n",
    "    If no pattern is found, returns an empty string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        patterns (list): List of regex patterns to match\n",
    "\n",
    "    Returns:\n",
    "        str: Text from the first pattern match to the end of the text, or empty string if no pattern is found\n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "\n",
    "    # Find the index of the first sentence matching any pattern\n",
    "    start_idx = -1\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if any(re.search(pattern, sentence) for pattern in patterns):\n",
    "            start_idx = i\n",
    "            break\n",
    "\n",
    "    # If no pattern was found, return an empty string\n",
    "    if start_idx == -1:\n",
    "        return \"\"\n",
    "\n",
    "    # Join the sentences from the matching sentence until the end\n",
    "    result_text = \" \".join(sentences[start_idx:])\n",
    "    return result_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts where no pattern was found: 54\n",
      "Percentage with no pattern found: 10.55%\n",
      "\n",
      "Introduction 1 (DOI: 10.1038/s41598-025-91206-6):\n",
      "Building rooftop extraction has been applied in various fields, such as cartography, urban planning, automatic driving, and intelligent city construction. Automatic building detection and extraction algorithms using high spatial resolution aerial images can provide precise location and geometry information, significantly reducing time, costs, and labor. Recently, deep learning algorithms, especially convolution neural networks (CNNs) and Transformer, have robust local or global feature extractio...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Introduction 2 (DOI: 10.1016/j.ijcce.2024.12.003):\n",
      "Problem: Modernizing and standardizing place names and addresses is a key challenge in the development of smart cities.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Introduction 3 (DOI: 10.1016/j.ipm.2025.104107):\n",
      "Distributed multivariate time series anomaly detection is widely-used in industrial equipment monitoring, financial risk management, and smart cities. Although Federated learning (FL) has garnered significant interest and achieved decent performance in various scenarios, most existing FL-based distributed anomaly detection methods still face challenges including: inadequate detection performance in global model, insufficient essential features extraction caused by the fragmentation of local time...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Add introduction text to the cleaned_data dictionary\n",
    "for item in data:\n",
    "    item[\"introduction\"] = extract_introduction_text(item[\"abstract\"], patterns)\n",
    "\n",
    "# Count abstracts with non-empty introductions\n",
    "abstracts_with_intros = [item for item in data if item[\"introduction\"].strip()]\n",
    "full_text_abstracts = [\n",
    "    item for item in data if item[\"introduction\"] == item[\"abstract\"]\n",
    "]\n",
    "\n",
    "# Statistics about abstracts where no pattern was found\n",
    "print(f\"Number of abstracts where no pattern was found: {len(full_text_abstracts)}\")\n",
    "print(\n",
    "    f\"Percentage with no pattern found: {len(full_text_abstracts) / len(data) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Display the first few introductory texts (up to 500 chars) along with their DOIs\n",
    "for i, item in enumerate(abstracts_with_intros[:3]):\n",
    "    print(f\"\\nIntroduction {i+1} (DOI: {item['doi']}):\")\n",
    "    intro = item[\"introduction\"]\n",
    "    print(f\"{intro[:500]}...\" if len(intro) > 500 else intro)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts processed: 512\n",
      "Number of abstracts with contributions extracted: 458\n",
      "Percentage with contributions extracted: 89.45%\n",
      "Average length of contributions: 1021.56 characters\n",
      "Article 1 (DOI: 10.1038/s41598-025-91206-6):\n",
      "To address these issues, this study developed a multi-scale global perceptron network based on Transformer and CNN using novel encoder-decoders for enhancing contextual representation of buildings. Specifically, an improved multi-head-attention encoder is employed by constructing multi-scale tokens to enhance global semantic correlations. Meanwhile, the context refinement decoder is developed and synergistically uses high-level semantic representation and shallow features to restore spatial deta...\n",
      "--------------------------------------------------------------------------------\n",
      "Article 2 (DOI: 10.1016/j.ijcce.2024.12.003):\n",
      "Purpose: This paper proposes a solution to address matching challenges, such as incomplete descriptions, reversed word order, and the diverse descriptions often found in Chinese addresses. Method: Leveraging the hierarchical structure of Chinese addresses, this study introduces the interactive address matching graph attention model (IAMGAM). In the IAMGAM, an attention-based feature interaction method (AFIM) is employed. To reflect the hierarchical nature of address elements, a directed graph is...\n",
      "--------------------------------------------------------------------------------\n",
      "Article 3 (DOI: 10.1016/j.ipm.2025.104107):\n",
      "To address these challenges, we propose an Unsupervised Federated Hypernetwork Method for Distributed Multivariate Time Series Anomaly Detection and Diagnosis (uFedHy-DisMTSADD). Specifically, we introduce a federated hypernetwork architecture that effectively mitigates the heterogeneity and fluctuations in distributed environments while protecting client data privacy. Then, we adopt the Series Conversion Normalization Transformer (SC Nor-Transformer) to tackle the timing bias due to model aggre...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Add contribution text to the cleaned_data dictionary\n",
    "for item in data:\n",
    "    item[\"contribution\"] = extract_text_from_pattern_to_end(item[\"abstract\"], patterns)\n",
    "\n",
    "# Count abstracts with non-empty contributions\n",
    "abstracts_with_contributions = [\n",
    "    item for item in data if item[\"contribution\"].strip()\n",
    "]\n",
    "\n",
    "# Display statistics about extracted text segments\n",
    "print(f\"Number of abstracts processed: {len(data)}\")\n",
    "print(\n",
    "    f\"Number of abstracts with contributions extracted: {len(abstracts_with_contributions)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage with contributions extracted: {len(abstracts_with_contributions) / len(data) * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Average length of contributions: {sum(len(item['contribution']) for item in abstracts_with_contributions) / max(1, len(abstracts_with_contributions)):.2f} characters\"\n",
    ")\n",
    "\n",
    "# Display the first few contribution texts (up to 500 chars) along with their DOIs\n",
    "for i, item in enumerate(abstracts_with_contributions[:3]):\n",
    "    print(f\"Article {i+1} (DOI: {item['doi']}):\")\n",
    "    contribution = item[\"contribution\"]\n",
    "    print(f\"{contribution[:500]}...\" if len(contribution) > 500 else contribution)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# For backward compatibility, you can still create the list if needed\n",
    "contribution_texts = [item[\"contribution\"] for item in data]\n",
    "non_empty_contributions = [text for text in contribution_texts if text.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified data back to a JSON file\n",
    "with open(\"../data/04_document_search_results_with_intros_and_contributions.json\", \"w\") as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
