{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles processed: 581\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file containing the search results\n",
    "# with open(\"../data/02_document_search_results.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "#     data = json.load(file)\n",
    "data = pd.read_json(\"../data/02_document_search_results.json\")\n",
    "\n",
    "\n",
    "# Print count for verification\n",
    "print(f\"Total articles processed: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Patterns \n",
    "Define patterns to differentiate from the introduction and contribution part of the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define expanded regex patterns to match descriptive sentences\n",
    "patterns = [\n",
    "    r\"\\b[Tt]his (article|work|study|paper|research|review|survey|chapter|viewpoint)\\b\",\n",
    "    r\"\\b[Ii]n this (work|study|paper|research|review|survey|chapter|viewpoint)\\b\",\n",
    "    r\"\\b[Ww]e (propose|introduce|present|develop|describe|demonstrate|report|discuss|analyze|examine|investigate|explore|evaluate|address|outline)\\b\",\n",
    "    r\"\\b[Ii]n this (manuscript|article|contribution|viewpoint|approach|framework|investigation|analysis|implementation)\\b\",\n",
    "    r\"\\b[Tt]he (article|paper|study|work|research|review|survey|manuscript|current study|present study|present work|current work)\\b\",\n",
    "    r\"\\b[Oo]ur (work|study|paper|chapter|viewpoint|research|approach|framework|method|system|contribution|focus|aim|objective|goal)\\b\",\n",
    "    r\"\\b[Tt]his (manuscript|contribution|investigation|viewpoint|analysis|implementation|approach|framework|method|system)\\b\",\n",
    "    r\"\\b[Tt]he (purpose|aim|goal|objective) of this (paper|work|study|research|article|chapter|manuscript)\\b\",\n",
    "    r\"\\b[Hh]ere(,)? we\\b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Introduction and Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_introduction_text(text, patterns):\n",
    "    \"\"\"\n",
    "    Extracts text from the beginning of the abstract until it reaches a sentence\n",
    "    that matches one of the specified patterns (typically where authors start\n",
    "    describing their specific work).\n",
    "\n",
    "    If no pattern is found, returns the entire text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        patterns (list): List of regex patterns to match\n",
    "\n",
    "    Returns:\n",
    "        str: Text until the first pattern match, or the entire text if no pattern is found\n",
    "    \"\"\"\n",
    "    # Handle empty text\n",
    "    if not text or not text.strip():\n",
    "        return text\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "\n",
    "    # Collect sentences until a pattern match is found\n",
    "    result_sentences = []\n",
    "    pattern_found = False\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Check if sentence matches any pattern\n",
    "        if any(re.search(pattern, sentence) for pattern in patterns):\n",
    "            # Only consider it a match if we're not on the first sentence\n",
    "            if i > 0:\n",
    "                pattern_found = True\n",
    "                break\n",
    "\n",
    "        result_sentences.append(sentence)\n",
    "\n",
    "    # If no pattern was found, return the original text\n",
    "    if not pattern_found or len(result_sentences) < 1 or len(\" \".join(result_sentences)) < 50:\n",
    "        return text\n",
    "\n",
    "    # Join the collected sentences back together with spaces\n",
    "    result_text = \" \".join(result_sentences)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_contribution(text, patterns):\n",
    "    \"\"\"\n",
    "    Extracts text from the first sentence matching any of the specified patterns\n",
    "    until the end of the text (typically where authors start describing their specific work).\n",
    "\n",
    "    If no pattern is found, returns an empty string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to process\n",
    "        patterns (list): List of regex patterns to match\n",
    "\n",
    "    Returns:\n",
    "        str: Text from the first pattern match to the end of the text, or empty string if no pattern is found\n",
    "    \"\"\"\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s\", text)\n",
    "\n",
    "    # Find the index of the first sentence matching any pattern\n",
    "    start_idx = -1\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if any(re.search(pattern, sentence) for pattern in patterns):\n",
    "            start_idx = i\n",
    "            break\n",
    "\n",
    "    # If no pattern was found, return the input text\n",
    "    if start_idx == -1:\n",
    "        return text\n",
    "\n",
    "    # Join the sentences from the matching sentence until the end\n",
    "    result_text = \" \".join(sentences[start_idx:])\n",
    "    return result_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts processed: 581\n",
      "Number of abstracts missing or empty: 1\n",
      "Number of abstracts with introductions extracted: 580\n",
      "Percentage with introductions extracted: 99.83%\n",
      "\n",
      "Number of abstracts where no pattern was found: 119\n",
      "Percentage with no pattern found: 20.52%\n",
      "\n",
      "Introduction 1 (DOI: 10.1016/j.ijcce.2024.12.003):\n",
      "Problem: Modernizing and standardizing place names and addresses is a key challenge in the development of smart cities. Purpose: This paper proposes a solution to address matching challenges, such as incomplete descriptions, reversed word order, and the diverse descriptions often found in Chinese addresses. Method: Leveraging the hierarchical structure of Chinese addresses, this study introduces the interactive address matching graph attention model (IAMGAM). In the IAMGAM, an attention-based fe...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Introduction 2 (DOI: 10.1016/j.sasc.2024.200176):\n",
      "3D modeling is actuality hired more and more by cities to improve urban planning and cultural protection. Sculptures in settlements are the main goal of this investigate into a novel 3D-Sculpture Architecture Estimation (3D-SAE) model. This model exploits Generative Adversarial Networks (GANs) to improve images, CNNs to extract features, and LDDNN–HGS-ROA, a Novel Lightweight Deep Neural Network mutual with the Hunger Games Search and Remora Optimization Method, to categorize images. The GAN-bas...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Introduction 3 (DOI: 10.1016/j.compenvurbsys.2025.102252):\n",
      "The importance of personal mobility data is widely recognized in various fields. However, the utilization of real personal mobility data raises privacy concerns. Therefore, it is crucial to generate pseudo personal mobility data that accurately reflects real-world mobility patterns while safeguarding user privacy. Nevertheless, existing methods for generating pseudo mobility data, mostly focusing on trip or trajectory generation, have limitations in capturing sufficient individual heterogeneity....\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Add introduction text to the cleaned_data dictionary\n",
    "for i, item in enumerate(data.to_dict('records')):\n",
    "    # Check if abstract exists before processing and is not NaN\n",
    "    if \"abstract\" in item and item[\"abstract\"] and not pd.isna(item[\"abstract\"]):\n",
    "        item[\"introduction\"] = extract_introduction_text(item[\"abstract\"], patterns)\n",
    "    else:\n",
    "        # Handle missing or NaN abstracts\n",
    "        item[\"introduction\"] = \"\"\n",
    "    \n",
    "    # Update the DataFrame with the modified record\n",
    "    data.loc[i, \"introduction\"] = item[\"introduction\"]\n",
    "\n",
    "# Count abstracts with non-empty introductions\n",
    "abstracts_with_intros = [item for item in data.to_dict('records') if item.get(\"introduction\", \"\").strip()]\n",
    "\n",
    "# Count how many abstracts are missing\n",
    "abstracts_missing = len(data[data[\"abstract\"].isna() | (data[\"abstract\"] == \"\")])\n",
    "\n",
    "# Count abstracts where no pattern was found (full text was returned as introduction)\n",
    "full_text_abstracts = data[\n",
    "    data[\"abstract\"].notna() & \n",
    "    (data[\"abstract\"] != \"\") & \n",
    "    (data[\"introduction\"] == data[\"abstract\"])\n",
    "].to_dict('records')\n",
    "\n",
    "# Display statistics\n",
    "print(f\"Number of abstracts processed: {len(data)}\")\n",
    "print(f\"Number of abstracts missing or empty: {abstracts_missing}\")\n",
    "print(f\"Number of abstracts with introductions extracted: {len(abstracts_with_intros)}\")\n",
    "print(f\"Percentage with introductions extracted: {len(abstracts_with_intros) / len(data) * 100:.2f}%\")\n",
    "\n",
    "# Statistics about abstracts where no pattern was found\n",
    "print(f\"\\nNumber of abstracts where no pattern was found: {len(full_text_abstracts)}\")\n",
    "if len(data) - abstracts_missing > 0:\n",
    "    print(f\"Percentage with no pattern found: {len(full_text_abstracts) / (len(data) - abstracts_missing) * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"Percentage with no pattern found: N/A (no valid abstracts)\")\n",
    "\n",
    "# Display the first few introductory texts (up to 500 chars) along with their DOIs\n",
    "for i, item in enumerate(abstracts_with_intros[:3]):\n",
    "    print(f\"\\nIntroduction {i+1} (DOI: {item.get('doi', 'No DOI')}):\")\n",
    "    intro = item[\"introduction\"]\n",
    "    print(f\"{intro[:500]}...\" if len(intro) > 500 else intro)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of abstracts processed: 581\n",
      "Number of abstracts with contributions extracted: 580\n",
      "Percentage with contributions extracted: 99.83%\n",
      "Average length of contributions: 1062.13 characters\n",
      "\n",
      "Article 1 (DOI: 10.1016/j.ijcce.2024.12.003):\n",
      "Purpose: This paper proposes a solution to address matching challenges, such as incomplete descriptions, reversed word order, and the diverse descriptions often found in Chinese addresses. Method: Leveraging the hierarchical structure of Chinese addresses, this study introduces the interactive address matching graph attention model (IAMGAM). In the IAMGAM, an attention-based feature interaction method (AFIM) is employed. To reflect the hierarchical nature of address elements, a directed graph is...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 2 (DOI: 10.1016/j.sasc.2024.200176):\n",
      "3D modeling is actuality hired more and more by cities to improve urban planning and cultural protection. Sculptures in settlements are the main goal of this investigate into a novel 3D-Sculpture Architecture Estimation (3D-SAE) model. This model exploits Generative Adversarial Networks (GANs) to improve images, CNNs to extract features, and LDDNN–HGS-ROA, a Novel Lightweight Deep Neural Network mutual with the Hunger Games Search and Remora Optimization Method, to categorize images. The GAN-bas...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Article 3 (DOI: 10.1016/j.compenvurbsys.2025.102252):\n",
      "Our method utilizes a deep generative model to generate heterogeneous individual life patterns, a variation inference model for inferring individual demographic characteristics, and a Bayesian-based approach for generating spatial choices considering individual demographic characteristics. Through our method, we have achieved generating realistic pseudo personal human mobility data - we evaluated the proposed method based on physical features – obeying common law of human mobility, activity feat...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Add contribution text to the DataFrame\n",
    "for i, item in enumerate(data.to_dict('records')):\n",
    "    # Check if abstract exists before processing and is not NaN\n",
    "    if \"abstract\" in item and item[\"abstract\"] and not pd.isna(item[\"abstract\"]):\n",
    "        item[\"contribution\"] = extract_contribution(item[\"abstract\"], patterns)\n",
    "    else:\n",
    "        # Handle missing or NaN abstracts\n",
    "        item[\"contribution\"] = \"\"\n",
    "    \n",
    "    # Update the DataFrame with the modified record\n",
    "    data.loc[i, \"contribution\"] = item[\"contribution\"]\n",
    "\n",
    "# Count abstracts with non-empty contributions\n",
    "abstracts_with_contributions = [\n",
    "    item for item in data.to_dict('records') if item.get(\"contribution\", \"\").strip()\n",
    "]\n",
    "\n",
    "# Display statistics about extracted text segments\n",
    "print(f\"Number of abstracts processed: {len(data)}\")\n",
    "print(\n",
    "    f\"Number of abstracts with contributions extracted: {len(abstracts_with_contributions)}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage with contributions extracted: {len(abstracts_with_contributions) / len(data) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Calculate average length only if there are contributions\n",
    "if abstracts_with_contributions:\n",
    "    average_length = sum(len(item['contribution']) for item in abstracts_with_contributions) / len(abstracts_with_contributions)\n",
    "    print(f\"Average length of contributions: {average_length:.2f} characters\")\n",
    "else:\n",
    "    print(\"Average length of contributions: N/A (no contributions found)\")\n",
    "\n",
    "# Display the first few contribution texts (up to 500 chars) along with their DOIs\n",
    "for i, item in enumerate(abstracts_with_contributions[:3]):\n",
    "    print(f\"\\nArticle {i+1} (DOI: {item.get('doi', 'No DOI')}):\")\n",
    "    contribution = item[\"contribution\"]\n",
    "    print(f\"{contribution[:500]}...\" if len(contribution) > 500 else contribution)\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# For backward compatibility, create list of contributions\n",
    "contribution_texts = [item.get(\"contribution\", \"\") for item in data.to_dict('records')]\n",
    "non_empty_contributions = [text for text in contribution_texts if text.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean records without introduction or contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with introductions or contributions: 580\n"
     ]
    }
   ],
   "source": [
    "# Clean records without introductions or contributions\n",
    "cleaned_data = data[(data[\"introduction\"].str.strip() != \"\") | (data[\"contribution\"].str.strip() != \"\")]\n",
    "print(f\"Number of records with introductions or contributions: {len(cleaned_data)}\")\n",
    "\n",
    "# For backward compatibility, create a list of dictionaries if needed\n",
    "cleaned_data_records = cleaned_data.to_dict('records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a JSON file using pandas\n",
    "cleaned_data.to_json(\"../data/04_document_search_results_with_intros_and_contributions.json\", orient=\"records\", indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the contributions in a json format\n",
    "with open(\"../data/processed/04_document_search_results_contributions.json\", \"w\") as file:\n",
    "    json.dump(contribution_texts, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
