{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and API Key Configuration\n",
    "Install the elsapy library and configure the API key required for authentication with Elsevier's APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elsclient import ElsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from a configuration file\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Initialize the ElsClient with the API key\n",
    "client = ElsClient(config['apikey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Elsevier Client\n",
    "Create an Elsevier client instance using your API key and configure the connection settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set the connection settings for the client\n",
    "client.base_url = 'https://api.elsevier.com/content/search/scopus'\n",
    "\n",
    "# Verify the client connection\n",
    "if client:\n",
    "    print(\"Client initialized successfully.\")\n",
    "else:\n",
    "    print(\"Failed to initialize client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and Analyze Search Results\n",
    "Process the search results to extract relevant information like titles, authors, journals, and citation counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the tags within the results\n",
    "def list_tags(results):\n",
    "    # Initialize lists to store keys\n",
    "    tags = set()  # Keys from search results\n",
    "    full_tags = set()  # Keys from abstract documents\n",
    "\n",
    "    # Extract keys from search results\n",
    "    for result in results:\n",
    "        if isinstance(result, dict):\n",
    "            for key in result.keys():\n",
    "                tags.add(key)\n",
    "    \n",
    "    # Sample a few results to get abstract document structure\n",
    "    # Limit to avoid too many API calls\n",
    "    sample_size = min(3, len(results))\n",
    "    \n",
    "    print(f\"Sampling {sample_size} documents to extract abstract document tags...\")\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        if i >= len(results):\n",
    "            break\n",
    "            \n",
    "        result = results[i]\n",
    "        if not isinstance(result, dict) or \"dc:identifier\" not in result:\n",
    "            continue\n",
    "            \n",
    "        scopus_id = result[\"dc:identifier\"].replace(\"SCOPUS_ID:\", \"\") if result[\"dc:identifier\"].startswith(\"SCOPUS_ID:\") else result[\"dc:identifier\"]\n",
    "        \n",
    "        try:\n",
    "            abs_doc = AbsDoc(scp_id=scopus_id)\n",
    "            if abs_doc.read(client):\n",
    "                # Recursively extract all keys from the abstract document data\n",
    "                def extract_keys(data, prefix=\"\"):\n",
    "                    if not isinstance(data, dict):\n",
    "                        return\n",
    "                        \n",
    "                    for key in data.keys():\n",
    "                        full_key = f\"{prefix}{key}\" if prefix else key\n",
    "                        full_tags.add(full_key)\n",
    "                        \n",
    "                        # Recursively process nested dictionaries\n",
    "                        if isinstance(data[key], dict):\n",
    "                            extract_keys(data[key], f\"{full_key}.\")\n",
    "                \n",
    "                extract_keys(abs_doc.data)\n",
    "                print(f\"Successfully extracted tags from document {i+1}\")\n",
    "            else:\n",
    "                print(f\"Failed to read abstract document for Scopus ID: {scopus_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {i+1} with Scopus ID {scopus_id}: {str(e)}\")\n",
    "            \n",
    "    return list(tags), list(full_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract relevant information from search results with filtering\n",
    "def extract_info(results, client, fetch_details=True, verbose=False):\n",
    "    extracted_data = []\n",
    "\n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"with_abstract\": 0,\n",
    "        \"with_keywords\": 0,\n",
    "        \"with_subject_areas\": 0,\n",
    "    }\n",
    "\n",
    "    # Use tqdm for progress tracking\n",
    "    for i, result in enumerate(tqdm(results, desc=\"Processing documents\")):\n",
    "        stats[\"total\"] += 1\n",
    "\n",
    "        # Basic metadata from search results\n",
    "        data = {\n",
    "            \"title\": result.get(\"dc:title\", \"\"),\n",
    "            \"authors\": result.get(\"dc:creator\"),\n",
    "            \"journal\": result.get(\"prism:publicationName\"),\n",
    "            \"doi\": result.get(\"prism:doi\"),\n",
    "            \"publication_date\": (\n",
    "                result.get(\"prism:coverDate\", \"\").split(\"-\")[0]\n",
    "                if result.get(\"prism:coverDate\")\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"document_type\": result.get(\"subtypeDescription\", \"\"),\n",
    "            \"prism:url\" : result.get(\"prism:url\"),\n",
    "            \"scopus_id\": (\n",
    "                result.get(\"dc:identifier\", \"\").replace(\"SCOPUS_ID:\", \"\")\n",
    "                if result.get(\"dc:identifier\")\n",
    "                else None\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # See if there's a description in the initial search results\n",
    "        if \"dc:description\" in result:\n",
    "            data[\"abstract\"] = result[\"dc:description\"]\n",
    "            stats[\"with_abstract\"] += 1\n",
    "\n",
    "        # If detailed information is requested and we have a scopus_id or doi\n",
    "        if fetch_details:\n",
    "            try:\n",
    "                # Try different methods to get the abstract\n",
    "                abstract_found = False\n",
    "\n",
    "                # Method 1: Try to get abstract from AbsDoc if scopus_id is available\n",
    "                if data[\"scopus_id\"] and not abstract_found:\n",
    "                    abs_doc = AbsDoc(scp_id=data[\"scopus_id\"])\n",
    "                    if abs_doc.read(client):\n",
    "                        # Try multiple paths for abstract\n",
    "                        if (\n",
    "                            \"coredata\" in abs_doc.data\n",
    "                            and \"dc:description\" in abs_doc.data[\"coredata\"]\n",
    "                        ):\n",
    "                            data[\"abstract\"] = abs_doc.data[\"coredata\"][\n",
    "                                \"dc:description\"\n",
    "                            ]\n",
    "                            abstract_found = True\n",
    "                            stats[\"with_abstract\"] += 1\n",
    "                        elif \"dc:description\" in abs_doc.data:\n",
    "                            data[\"abstract\"] = abs_doc.data[\"dc:description\"]\n",
    "                            abstract_found = True\n",
    "                            stats[\"with_abstract\"] += 1\n",
    "\n",
    "                        # Extract keywords\n",
    "                        if \"authkeywords\" in abs_doc.data:\n",
    "                            keywords = abs_doc.data[\"authkeywords\"]\n",
    "                            keyword_list = []\n",
    "\n",
    "                            # Process the nested structure to extract keyword values\n",
    "                            if \"author-keyword\" in keywords:\n",
    "                                author_kws = keywords[\"author-keyword\"]\n",
    "\n",
    "                                if isinstance(author_kws, list):\n",
    "                                    for kw in author_kws:\n",
    "                                        if isinstance(kw, dict) and \"$\" in kw:\n",
    "                                            keyword_list.append(kw[\"$\"])\n",
    "                                elif isinstance(author_kws, dict) and \"$\" in author_kws:\n",
    "                                    keyword_list.append(author_kws[\"$\"])\n",
    "\n",
    "                            if keyword_list:\n",
    "                                data[\"author_keywords\"] = keyword_list\n",
    "                                stats[\"with_keywords\"] += 1\n",
    "\n",
    "                        # Extract subject areas\n",
    "                        if \"subject-areas\" in abs_doc.data:\n",
    "                            subject_areas = abs_doc.data[\"subject-areas\"].get(\n",
    "                                \"subject-area\", []\n",
    "                            )\n",
    "                            if isinstance(subject_areas, list):\n",
    "                                data[\"subject_areas\"] = [\n",
    "                                    area.get(\"$\")\n",
    "                                    for area in subject_areas\n",
    "                                    if \"$\" in area\n",
    "                                ]\n",
    "                            elif (\n",
    "                                isinstance(subject_areas, dict) and \"$\" in subject_areas\n",
    "                            ):\n",
    "                                data[\"subject_areas\"] = [subject_areas[\"$\"]]\n",
    "\n",
    "                            if \"subject_areas\" in data:\n",
    "                                stats[\"with_subject_areas\"] += 1\n",
    "\n",
    "                # Avoid hitting rate limits\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Error retrieving details for document {i+1}: {str(e)}\")\n",
    "                    print(\n",
    "                        f\"Failed document: {data['title']} (DOI: {data.get('doi')}, ScopusID: {data['scopus_id']})\"\n",
    "                    )\n",
    "\n",
    "        extracted_data.append(data)\n",
    "\n",
    "    # Print capture statistics\n",
    "    print(\"\\nData Capture Statistics:\")\n",
    "    print(f\"Total documents processed: {stats['total']}\")\n",
    "    print(\n",
    "        f\"Documents with abstracts: {stats['with_abstract']} ({stats['with_abstract']*100/stats['total']:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Documents with keywords: {stats['with_keywords']} ({stats['with_keywords']*100/stats['total']:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Documents with subject areas: {stats['with_subject_areas']} ({stats['with_subject_areas']*100/stats['total']:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    return extracted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving detailed information for documents...\n",
      "Total documents: 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 512/512 [12:04<00:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Capture Statistics:\n",
      "Total documents processed: 512\n",
      "Documents with abstracts: 512 (100.0%)\n",
      "Documents with keywords: 475 (92.8%)\n",
      "Documents with subject areas: 475 (92.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract information from document search results with filtering and details\n",
    "print(\"Retrieving detailed information for documents...\")\n",
    "# Read doc_info from JSON file\n",
    "with open(\"../data/01_scopus_results.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "# tags, full_tags = list_tags(results)\n",
    "# print(f\"Tags in the results: {tags}\")\n",
    "# print(f\"Full tags in the results: {full_tags}\")\n",
    "\n",
    "print(f\"Total documents: {len(results)}\")\n",
    "\n",
    "doc_info = extract_info(results, client, fetch_details=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Search Results\n",
    "Export the search results to CSV or JSON format for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export results to CSV with dynamic fields\n",
    "def export_to_csv(data, filename):\n",
    "    # Get all possible keys across all dictionaries\n",
    "    all_keys = set()\n",
    "    for item in data:\n",
    "        all_keys.update(item.keys())\n",
    "\n",
    "    # Convert to sorted list for consistent column order\n",
    "    fieldnames = sorted(list(all_keys))\n",
    "\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data)\n",
    "\n",
    "    print(f\"CSV exported with {len(data)} records\")\n",
    "\n",
    "\n",
    "# Function to export results to JSON\n",
    "def export_to_json(data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"JSON exported with {len(data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV exported with 512 records\n",
      "JSON exported with 512 records\n",
      "Search results exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Export document search results to CSV and JSON\n",
    "export_to_csv(doc_info, \"../data/02_document_search_results.csv\")\n",
    "export_to_json(doc_info, \"../data/02_document_search_results.json\")\n",
    "\n",
    "print(\"Search results exported successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
