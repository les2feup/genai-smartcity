{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and API Key Configuration\n",
    "Install the elsapy library and configure the API key required for authentication with Elsevier's APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elsclient import ElsClient\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key from a configuration file\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Initialize the ElsClient with the API key\n",
    "client = ElsClient(config['apikey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Elsevier Client\n",
    "Create an Elsevier client instance using your API key and configure the connection settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Set the connection settings for the client\n",
    "client.base_url = 'https://api.elsevier.com/content/search/scopus'\n",
    "\n",
    "# Verify the client connection\n",
    "if client:\n",
    "    print(\"Client initialized successfully.\")\n",
    "else:\n",
    "    print(\"Failed to initialize client.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and Analyze Search Results\n",
    "Process the search results to extract relevant information like titles, authors, journals, and citation counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the tags within the results\n",
    "def list_tags(results):\n",
    "    # Initialize lists to store keys\n",
    "    tags = set()  # Keys from search results\n",
    "    full_tags = set()  # Keys from abstract documents\n",
    "\n",
    "    # Extract keys from search results\n",
    "    for result in results:\n",
    "        if isinstance(result, dict):\n",
    "            for key in result.keys():\n",
    "                tags.add(key)\n",
    "    \n",
    "    # Sample a few results to get abstract document structure\n",
    "    # Limit to avoid too many API calls\n",
    "    sample_size = min(3, len(results))\n",
    "    \n",
    "    print(f\"Sampling {sample_size} documents to extract abstract document tags...\")\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        if i >= len(results):\n",
    "            break\n",
    "            \n",
    "        result = results[i]\n",
    "        if not isinstance(result, dict) or \"dc:identifier\" not in result:\n",
    "            continue\n",
    "            \n",
    "        scopus_id = result[\"dc:identifier\"].replace(\"SCOPUS_ID:\", \"\") if result[\"dc:identifier\"].startswith(\"SCOPUS_ID:\") else result[\"dc:identifier\"]\n",
    "        \n",
    "        try:\n",
    "            abs_doc = AbsDoc(scp_id=scopus_id)\n",
    "            if abs_doc.read(client):\n",
    "                # Recursively extract all keys from the abstract document data\n",
    "                def extract_keys(data, prefix=\"\"):\n",
    "                    if not isinstance(data, dict):\n",
    "                        return\n",
    "                        \n",
    "                    for key in data.keys():\n",
    "                        full_key = f\"{prefix}{key}\" if prefix else key\n",
    "                        full_tags.add(full_key)\n",
    "                        \n",
    "                        # Recursively process nested dictionaries\n",
    "                        if isinstance(data[key], dict):\n",
    "                            extract_keys(data[key], f\"{full_key}.\")\n",
    "                \n",
    "                extract_keys(abs_doc.data)\n",
    "                print(f\"Successfully extracted tags from document {i+1}\")\n",
    "            else:\n",
    "                print(f\"Failed to read abstract document for Scopus ID: {scopus_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {i+1} with Scopus ID {scopus_id}: {str(e)}\")\n",
    "            \n",
    "    return list(tags), list(full_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(results, client, fetch_details=True, verbose=False):\n",
    "    # Convert results to DataFrame if not already\n",
    "    if not isinstance(results, pd.DataFrame):\n",
    "        df_results = pd.DataFrame(results)\n",
    "    else:\n",
    "        df_results = results\n",
    "    \n",
    "    # Statistics tracking\n",
    "    stats = {\n",
    "        \"total\": 0,\n",
    "        \"with_abstract\": 0,\n",
    "        \"with_keywords\": 0,\n",
    "        \"with_subject_areas\": 0,\n",
    "    }\n",
    "    \n",
    "    # Function to process a single document\n",
    "    def process_document(result):\n",
    "        nonlocal stats\n",
    "        stats[\"total\"] += 1\n",
    "        \n",
    "        # If result is a Series (row of DataFrame), convert to dict\n",
    "        if isinstance(result, pd.Series):\n",
    "            result = result.to_dict()\n",
    "        \n",
    "        # Basic metadata from search results\n",
    "        data = {\n",
    "            \"title\": result.get(\"dc:title\", \"\"),\n",
    "            \"authors\": result.get(\"dc:creator\"),\n",
    "            \"journal\": result.get(\"prism:publicationName\"),\n",
    "            \"doi\": result.get(\"prism:doi\"),\n",
    "            \"publication_date\": (\n",
    "                result.get(\"prism:coverDate\", \"\").split(\"-\")[0]\n",
    "                if result.get(\"prism:coverDate\")\n",
    "                else \"\"\n",
    "            ),\n",
    "            \"document_type\": result.get(\"subtypeDescription\", \"\"),\n",
    "            \"prism:url\": result.get(\"prism:url\"),\n",
    "            \"scopus_id\": (\n",
    "                result.get(\"dc:identifier\", \"\").replace(\"SCOPUS_ID:\", \"\")\n",
    "                if result.get(\"dc:identifier\")\n",
    "                else None\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # See if there's a description in the initial search results\n",
    "        if \"dc:description\" in result:\n",
    "            data[\"abstract\"] = result[\"dc:description\"]\n",
    "            stats[\"with_abstract\"] += 1\n",
    "\n",
    "        # If detailed information is requested and we have a scopus_id or doi\n",
    "        if fetch_details:\n",
    "            try:\n",
    "                # Try different methods to get the abstract\n",
    "                abstract_found = \"abstract\" in data\n",
    "                \n",
    "                # Method 1: Try to get abstract from AbsDoc if scopus_id is available\n",
    "                if data[\"scopus_id\"] and not abstract_found:\n",
    "                    abs_doc = AbsDoc(scp_id=data[\"scopus_id\"])\n",
    "                    if abs_doc.read(client):\n",
    "                        # Try multiple paths for abstract\n",
    "                        if (\n",
    "                            \"coredata\" in abs_doc.data\n",
    "                            and \"dc:description\" in abs_doc.data[\"coredata\"]\n",
    "                        ):\n",
    "                            data[\"abstract\"] = abs_doc.data[\"coredata\"][\"dc:description\"]\n",
    "                            abstract_found = True\n",
    "                            stats[\"with_abstract\"] += 1\n",
    "                        elif \"dc:description\" in abs_doc.data:\n",
    "                            data[\"abstract\"] = abs_doc.data[\"dc:description\"]\n",
    "                            abstract_found = True\n",
    "                            stats[\"with_abstract\"] += 1\n",
    "\n",
    "                        # Extract keywords\n",
    "                        if \"authkeywords\" in abs_doc.data:\n",
    "                            keywords = abs_doc.data[\"authkeywords\"]\n",
    "                            keyword_list = []\n",
    "\n",
    "                            # Process the nested structure to extract keyword values\n",
    "                            if \"author-keyword\" in keywords:\n",
    "                                author_kws = keywords[\"author-keyword\"]\n",
    "\n",
    "                                if isinstance(author_kws, list):\n",
    "                                    for kw in author_kws:\n",
    "                                        if isinstance(kw, dict) and \"$\" in kw:\n",
    "                                            keyword_list.append(kw[\"$\"])\n",
    "                                elif isinstance(author_kws, dict) and \"$\" in author_kws:\n",
    "                                    keyword_list.append(author_kws[\"$\"])\n",
    "\n",
    "                            if keyword_list:\n",
    "                                data[\"author_keywords\"] = keyword_list\n",
    "                                stats[\"with_keywords\"] += 1\n",
    "\n",
    "                        # Extract subject areas\n",
    "                        if \"subject-areas\" in abs_doc.data:\n",
    "                            subject_areas = abs_doc.data[\"subject-areas\"].get(\n",
    "                                \"subject-area\", []\n",
    "                            )\n",
    "                            if isinstance(subject_areas, list):\n",
    "                                data[\"subject_areas\"] = [\n",
    "                                    area.get(\"$\")\n",
    "                                    for area in subject_areas\n",
    "                                    if \"$\" in area\n",
    "                                ]\n",
    "                            elif (\n",
    "                                isinstance(subject_areas, dict) and \"$\" in subject_areas\n",
    "                            ):\n",
    "                                data[\"subject_areas\"] = [subject_areas[\"$\"]]\n",
    "\n",
    "                            if \"subject_areas\" in data:\n",
    "                                stats[\"with_subject_areas\"] += 1\n",
    "\n",
    "                # Avoid hitting rate limits\n",
    "                time.sleep(0.2)\n",
    "\n",
    "            except Exception as e:\n",
    "                if verbose:\n",
    "                    print(f\"Error retrieving details for document: {str(e)}\")\n",
    "                    print(\n",
    "                        f\"Failed document: {data['title']} (DOI: {data.get('doi')}, ScopusID: {data['scopus_id']})\"\n",
    "                    )\n",
    "\n",
    "        return data\n",
    "    \n",
    "    # Process documents with pandas progress bar\n",
    "    print(\"Processing documents...\")\n",
    "    extracted_data = list(df_results.progress_apply(process_document, axis=1))\n",
    "    \n",
    "    # Print capture statistics\n",
    "    print(\"\\nData Capture Statistics:\")\n",
    "    print(f\"Total documents processed: {stats['total']}\")\n",
    "    print(\n",
    "        f\"Documents with abstracts: {stats['with_abstract']} ({stats['with_abstract']*100/stats['total']:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Documents with keywords: {stats['with_keywords']} ({stats['with_keywords']*100/stats['total']:.1f}%)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Documents with subject areas: {stats['with_subject_areas']} ({stats['with_subject_areas']*100/stats['total']:.1f}%)\"\n",
    "    )\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72aa704d35ec48728ac0d0ffb69bc231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Capture Statistics:\n",
      "Total documents processed: 973\n",
      "Documents with abstracts: 971 (99.8%)\n",
      "Documents with keywords: 893 (91.8%)\n",
      "Documents with subject areas: 893 (91.8%)\n"
     ]
    }
   ],
   "source": [
    "# Extract information from document search results with filtering and details\n",
    "\n",
    "# Read doc_info from JSON file\n",
    "with open(\"../data/01_scopus_results.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "# Convert to DataFrame for pandas processing\n",
    "results_df = pd.DataFrame(results)\n",
    "doc_info = extract_info(results_df, client, fetch_details=True, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created DataFrame with 973 records and 11 columns\n",
      "\n",
      "Columns in the document info DataFrame:\n",
      "['title', 'authors', 'journal', 'doi', 'publication_date', 'document_type', 'prism:url', 'scopus_id', 'abstract', 'author_keywords', 'subject_areas']\n",
      "Cleaned title column\n",
      "Cleaned abstract column\n",
      "\n",
      "Sample of cleaned data:\n",
      "                                               title  \\\n",
      "0  An interactive address matching method based o...   \n",
      "1  Intelligent pattern design using 3D modelling ...   \n",
      "2  Digital divides in scene recognition: uncoveri...   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Problem: Modernizing and standardizing place n...   \n",
      "1  3D modeling is actuality hired more and more b...   \n",
      "2  Automatic scene classification has application...   \n",
      "\n",
      "                                     author_keywords  \n",
      "0  [Address matching, Attention-based feature int...  \n",
      "1  [3D modelling, Urban sculpture designing, 3D-S...  \n",
      "2                                                NaN  \n",
      "\n",
      "Updated original doc_info with cleaned data\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Analyze and clean the document data\n",
    "\n",
    "# Create a DataFrame from doc_info if it's a list\n",
    "if isinstance(doc_info, list):\n",
    "    doc_info_df = pd.DataFrame(doc_info)\n",
    "    print(f\"Created DataFrame with {len(doc_info_df)} records and {len(doc_info_df.columns)} columns\")\n",
    "else:\n",
    "    doc_info_df = doc_info\n",
    "    print(f\"Using existing DataFrame with {len(doc_info_df)} records\")\n",
    "\n",
    "# Display the columns to understand the structure\n",
    "print(\"\\nColumns in the document info DataFrame:\")\n",
    "print(doc_info_df.columns.tolist())\n",
    "\n",
    "# Function to clean text by removing unwanted characters and weird symbols\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Remove Unicode characters like \\u00a0 (non-breaking space)\n",
    "    text = text.replace('\\u00a0', ' ')\n",
    "    \n",
    "    # Remove XML/HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Other common replacements\n",
    "    text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')\n",
    "    \n",
    "    # Trim whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning to text columns\n",
    "columns_to_clean = ['title', 'abstract']\n",
    "if 'author_keywords' in doc_info_df.columns:\n",
    "    # For lists of keywords, we need to clean each item\n",
    "    doc_info_df['author_keywords'] = doc_info_df['author_keywords'].apply(\n",
    "        lambda keywords: [clean_text(k) for k in keywords] if isinstance(keywords, list) else keywords\n",
    "    )\n",
    "\n",
    "# Apply the clean_text function to text columns\n",
    "for col in columns_to_clean:\n",
    "    if col in doc_info_df.columns:\n",
    "        doc_info_df[col] = doc_info_df[col].apply(clean_text)\n",
    "        print(f\"Cleaned {col} column\")\n",
    "\n",
    "# Show sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "sample_cols = [col for col in columns_to_clean if col in doc_info_df.columns]\n",
    "if 'author_keywords' in doc_info_df.columns:\n",
    "    sample_cols.append('author_keywords')\n",
    "    \n",
    "if sample_cols:\n",
    "    print(doc_info_df[sample_cols].head(3))\n",
    "\n",
    "# Update the original doc_info list with the cleaned data\n",
    "if isinstance(doc_info, list):\n",
    "    doc_info = doc_info_df.to_dict('records')\n",
    "    print(\"\\nUpdated original doc_info with cleaned data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Search Results\n",
    "Export the search results to CSV or JSON format for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to export results to CSV with dynamic fields\n",
    "def export_to_csv(data, filename):\n",
    "    # Get all possible keys across all dictionaries\n",
    "    all_keys = set()\n",
    "    for item in data:\n",
    "        all_keys.update(item.keys())\n",
    "\n",
    "    # Convert to sorted list for consistent column order\n",
    "    fieldnames = sorted(list(all_keys))\n",
    "\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, fieldnames=fieldnames)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(data)\n",
    "\n",
    "    print(f\"CSV exported with {len(data)} records\")\n",
    "\n",
    "\n",
    "# Function to export results to JSON\n",
    "def export_to_json(data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"JSON exported with {len(data)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON exported with 973 records\n",
      "Search results exported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Export document search results to CSV and JSON\n",
    "# export_to_csv(doc_info, \"../data/02_document_search_results.csv\")\n",
    "export_to_json(doc_info, \"../data/02_document_search_results.json\")\n",
    "\n",
    "print(\"Search results exported successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
