{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import spacy\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "\n",
    "# Initial configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load models (using lighter versions for demonstration)\n",
    "sbert_model = SentenceTransformer(\n",
    "    \"paraphrase-MiniLM-L6-v2\", device=DEVICE\n",
    ")  # Lighter model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Using small spaCy model\n",
    "\n",
    "# Expanded word groups\n",
    "group_1 = {\n",
    "    \"Economy\",\n",
    "    \"Business\",\n",
    "    \"Economic Management\",\n",
    "    \"Innovation Policy\",\n",
    "    \"Socioeconomics\",\n",
    "    \"Governance\",\n",
    "    \"Public Services\",\n",
    "    \"Public Policies\",\n",
    "    \"Urban Planning\",\n",
    "    \"Social Equity\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Living\",\n",
    "    \"Housing\",\n",
    "    \"Tourism\",\n",
    "    \"Culture\",\n",
    "    \"Buildings\",\n",
    "    \"Education\",\n",
    "    \"Healthcare\",\n",
    "    \"Emergency Safety\",\n",
    "    \"Mobility\",\n",
    "    \"Traffic Management\",\n",
    "    \"Transportation Systems\",\n",
    "    \"Electric Vehicles\",\n",
    "    \"Public Transit\",\n",
    "    \"People\",\n",
    "    \"Citizens\",\n",
    "    \"Community Engagement\",\n",
    "    \"Learning and Teaching\",\n",
    "    \"Waste Management\",\n",
    "    \"Pollution Control\",\n",
    "    \"Resource Conservation\",\n",
    "    \"Energy Management\",\n",
    "    \"Smart Grids\",\n",
    "    \"Lightning\",\n",
    "    \"Air Quality\",\n",
    "    \"Water Quality\",\n",
    "    \"Green Spaces\",\n",
    "}\n",
    "\n",
    "group_2 = {\n",
    "    \"gpt\",\n",
    "    \"bert\",\n",
    "    \"llama\",\n",
    "    \"dall\",\n",
    "    \"dall-e\",\n",
    "    \"slm\",\n",
    "    \"small language model\",\n",
    "    \"gan\",\n",
    "    \"generative adversarial network\",\n",
    "    \"transformers\",\n",
    "    \"transformer model\",\n",
    "    \"large language model\",\n",
    "    \"llm\",\n",
    "    \"vae\",\n",
    "    \"diffusion model\",\n",
    "    \"neural language model\",\n",
    "    \"foundation model\",\n",
    "    \"multimodal model\",\n",
    "    \"language model\",\n",
    "    \"generative ai\",\n",
    "    \"generative model\",\n",
    "    \"ai model\",\n",
    "    \"agent\",\n",
    "    \"agents\",\n",
    "}\n",
    "\n",
    "\n",
    "class EnhancedSemanticSearch:\n",
    "    def __init__(self):\n",
    "        # Pre-compute embeddings for the groups\n",
    "        self.group1_embeddings = self._precompute_embeddings(group_1)\n",
    "        self.group2_embeddings = self._precompute_embeddings(group_2)\n",
    "\n",
    "    def _precompute_embeddings(self, terms):\n",
    "        \"\"\"Pre-computes embeddings for group terms\"\"\"\n",
    "        return sbert_model.encode(list(terms), convert_to_tensor=True)\n",
    "\n",
    "    def _calculate_similarity(self, text, group_embeddings):\n",
    "        \"\"\"Calculates similarity between text and group terms\"\"\"\n",
    "        text_embedding = sbert_model.encode(text, convert_to_tensor=True)\n",
    "        cos_scores = util.pytorch_cos_sim(text_embedding, group_embeddings)[0]\n",
    "        return torch.max(cos_scores).item()\n",
    "\n",
    "    def _find_key_terms(self, text, group_terms):\n",
    "        \"\"\"Finds group terms present in the text using improved matching\"\"\"\n",
    "        found_terms = []\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        # 1. Try exact matches first\n",
    "        for term in group_terms:\n",
    "            term_lower = term.lower()\n",
    "            if term_lower in text_lower:\n",
    "                found_terms.append(term)\n",
    "\n",
    "        # 2. If no exact matches, try word boundary matches (whole words only)\n",
    "        if not found_terms:\n",
    "            text_tokens = set(re.findall(r\"\\b\\w+\\b\", text_lower))\n",
    "            for term in group_terms:\n",
    "                # For multi-word terms, split and check if all words are present\n",
    "                term_lower = term.lower()\n",
    "                term_tokens = set(re.findall(r\"\\b\\w+\\b\", term_lower))\n",
    "\n",
    "                # If all tokens from the term are in the text tokens, it's a match\n",
    "                if term_tokens.issubset(text_tokens):\n",
    "                    found_terms.append(term)\n",
    "\n",
    "        # 3. Try fuzzy matching for close matches\n",
    "        if not found_terms:\n",
    "            doc = nlp(text)\n",
    "            for term in group_terms:\n",
    "                term_lower = term.lower()\n",
    "                # Use spaCy to check for named entities or noun chunks that might match\n",
    "                for ent in doc.ents:\n",
    "                    similarity = self._calculate_similarity(\n",
    "                        ent.text,\n",
    "                        sbert_model.encode([term_lower], convert_to_tensor=True),\n",
    "                    )\n",
    "                    if similarity > 0.7:  # High threshold for fuzzy matching\n",
    "                        found_terms.append(term)\n",
    "                        break  # Found a match for this term\n",
    "\n",
    "        return found_terms\n",
    "\n",
    "    def analyze_abstracts(self, abstract_data, similarity_threshold=0.5):\n",
    "        \"\"\"Analyzes abstract data with a hybrid approach (semantic similarity + term matching)\"\"\"\n",
    "        results = {\n",
    "            \"high_similarity\": [],\n",
    "            \"term_matches\": [],\n",
    "            \"group1_count\": 0,\n",
    "            \"group2_count\": 0,\n",
    "            \"bridges\": [],\n",
    "        }\n",
    "\n",
    "        for item in tqdm(abstract_data, desc=\"Analyzing abstracts\"):\n",
    "            # Get the introduction text from the dictionary\n",
    "            text = item[\"introduction\"]\n",
    "            doi = item.get(\"doi\", \"Unknown\")\n",
    "\n",
    "            # Skip empty introductions\n",
    "            if not text or not isinstance(text, str):\n",
    "                continue\n",
    "\n",
    "            # Semantic analysis\n",
    "            sim1 = self._calculate_similarity(text, self.group1_embeddings)\n",
    "            sim2 = self._calculate_similarity(text, self.group2_embeddings)\n",
    "\n",
    "            # Direct term matching\n",
    "            found_group1 = self._find_key_terms(text, group_1)\n",
    "            found_group2 = self._find_key_terms(text, group_2)\n",
    "\n",
    "            # Store analysis results in the item\n",
    "            item[\"semantic_analysis\"] = {\n",
    "                \"group1_similarity\": sim1,\n",
    "                \"group2_similarity\": sim2,\n",
    "                \"group1_terms\": found_group1,\n",
    "                \"group2_terms\": found_group2,\n",
    "            }\n",
    "\n",
    "            # Record results\n",
    "            if sim1 >= similarity_threshold or found_group1:\n",
    "                results[\"group1_count\"] += 1\n",
    "\n",
    "            if sim2 >= similarity_threshold or found_group2:\n",
    "                results[\"group2_count\"] += 1\n",
    "\n",
    "            if (sim1 >= similarity_threshold or found_group1) and (\n",
    "                sim2 >= similarity_threshold or found_group2\n",
    "            ):\n",
    "                entry = {\n",
    "                    \"text\": text,\n",
    "                    \"doi\": doi,\n",
    "                    \"group1_terms\": found_group1,\n",
    "                    \"group2_terms\": found_group2,\n",
    "                    \"group1_sim\": sim1,\n",
    "                    \"group2_sim\": sim2,\n",
    "                }\n",
    "\n",
    "                if sim1 >= similarity_threshold and sim2 >= similarity_threshold:\n",
    "                    results[\"high_similarity\"].append(entry)\n",
    "                else:\n",
    "                    results[\"term_matches\"].append(entry)\n",
    "\n",
    "                # Extract connecting terms\n",
    "                doc = nlp(text)\n",
    "                connectors = [\n",
    "                    token.text.lower()\n",
    "                    for token in doc\n",
    "                    if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\"]\n",
    "                    and token.text.lower() not in [t.lower() for t in group_1]\n",
    "                    and token.text.lower() not in [t.lower() for t in group_2]\n",
    "                ]\n",
    "                results[\"bridges\"].extend(connectors)\n",
    "\n",
    "        # Analyze most common connecting terms\n",
    "        if results[\"bridges\"]:\n",
    "            bridge_counts = Counter(results[\"bridges\"])\n",
    "            results[\"top_bridges\"] = bridge_counts.most_common(10)\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "# Get abstracts with non-empty introductions\n",
    "abstracts_with_intros = [\n",
    "    item for item in cleaned_abstract_data if item[\"introduction\"].strip()\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"\\nAnalyzing {len(abstracts_with_intros)} abstracts with non-empty introductions...\"\n",
    ")\n",
    "\n",
    "# Create analyzer and analyze abstracts\n",
    "analyzer = EnhancedSemanticSearch()\n",
    "\n",
    "analysis = analyzer.analyze_abstracts(abstracts_with_intros, similarity_threshold=0.3)\n",
    "\n",
    "print(\"\\nüìä Enhanced Results:\")\n",
    "print(f\"Texts with high semantic similarity: {len(analysis['high_similarity'])}\")\n",
    "print(f\"Texts with term matching: {len(analysis['term_matches'])}\")\n",
    "print(f\"Mentions of Group 1 (Smart City domains): {analysis['group1_count']}\")\n",
    "print(f\"Mentions of Group 2 (AI/ML technologies): {analysis['group2_count']}\")\n",
    "\n",
    "if \"top_bridges\" in analysis:\n",
    "    print(\"\\nüåâ Most Frequent Connecting Terms:\")\n",
    "    for term, count in analysis[\"top_bridges\"]:\n",
    "        print(f\"- {term} (x{count})\")\n",
    "\n",
    "print(\"\\nüîç Relevant Examples:\")\n",
    "for i, item in enumerate(analysis[\"high_similarity\"][:5], 1):  # Show first 5 examples\n",
    "    print(f\"\\nüìå Example {i} (DOI: {item['doi']}):\")\n",
    "    print(f\"Text: {item['text'][:200]}...\")\n",
    "    print(f\"Group 1 Terms: {', '.join(item['group1_terms'])}\")\n",
    "    print(f\"Group 2 Terms: {', '.join(item['group2_terms'])}\")\n",
    "    print(f\"Similarity G1: {item['group1_sim']:.2f}, G2: {item['group2_sim']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
