{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content for 325 articles...\n",
      "\n",
      "Processing document 1/325...\n",
      "Title: Instruction-Tuning Llama-3-8B Excels in City-Scale Mobility Prediction\n",
      "DOI: 10.1145/3681771.3699908\n",
      "Requesting XML content...\n",
      "Failed to retrieve XML content: 404\n",
      "Error response: <service-error><status><statusCode>RESOURCE_NOT_FOUND</statusCode><statusText>The resource specified cannot be found.</statusText></status></service-error>...\n",
      "\n",
      "Processing document 2/325...\n",
      "Title: Large language model as parking planning agent in the context of mixed period of autonomous vehicles and Human-Driven vehicles\n",
      "DOI: 10.1016/j.scs.2024.105940\n",
      "Requesting XML content...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/vmm5p4s55yl7nkbdgy26xn400000gn/T/ipykernel_46984/1210933624.py:110: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  section_title_elem = section_elem.find('./ce:section-title', ns) or section_elem.find('./section-title', ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content saved to article_content/extracted_text/Large_language_model_as_parking_planning_agent_in__2024.json\n",
      "\n",
      "Processing document 3/325...\n",
      "Title: Evaluating a global citizenship course on developing business students' AI literacy skills\n",
      "DOI: 10.1108/978-1-83608-852-320241010\n",
      "Requesting XML content...\n",
      "Failed to retrieve XML content: 404\n",
      "Error response: <service-error><status><statusCode>RESOURCE_NOT_FOUND</statusCode><statusText>The resource specified cannot be found.</statusText></status></service-error>...\n",
      "\n",
      "Processing document 4/325...\n",
      "Title: Arabic Opinion Classification of Customer Service Conversations Using Data Augmentation and Artificial Intelligence\n",
      "DOI: 10.3390/bdcc8120196\n",
      "Requesting XML content...\n",
      "Failed to retrieve XML content: 404\n",
      "Error response: <service-error><status><statusCode>RESOURCE_NOT_FOUND</statusCode><statusText>The resource specified cannot be found.</statusText></status></service-error>...\n",
      "\n",
      "Processing document 5/325...\n",
      "Title: Integrating Large Language Models and Optimization in Semi- Structured Decision Making: Methodology and a Case Study\n",
      "DOI: 10.3390/a17120582\n",
      "Requesting XML content...\n",
      "Failed to retrieve XML content: 404\n",
      "Error response: <service-error><status><statusCode>RESOURCE_NOT_FOUND</statusCode><statusText>The resource specified cannot be found.</statusText></status></service-error>...\n",
      "\n",
      "Processing document 6/325...\n",
      "Title: Investigating the Performance of Open-Vocabulary Classification Algorithms for Pathway and Surface Material Detection in Urban Environments\n",
      "DOI: 10.3390/ijgi13120422\n",
      "Requesting XML content...\n",
      "Failed to retrieve XML content: 404\n",
      "Error response: <service-error><status><statusCode>RESOURCE_NOT_FOUND</statusCode><statusText>The resource specified cannot be found.</statusText></status></service-error>...\n",
      "\n",
      "Processing document 7/325...\n",
      "Title: Artificial Intelligence-Enabled Metaverse for Sustainable Smart Cities: Technologies, Applications, Challenges, and Future Directions\n",
      "DOI: 10.3390/electronics13244874\n",
      "Requesting XML content...\n",
      "Failed to retrieve XML content: 404\n",
      "Error response: <service-error><status><statusCode>RESOURCE_NOT_FOUND</statusCode><statusText>The resource specified cannot be found.</statusText></status></service-error>...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[148]\u001b[39m\u001b[32m, line 210\u001b[39m\n",
      "\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# For testing, just use a few documents first\u001b[39;00m\n",
      "\u001b[32m    209\u001b[39m test_docs = doc_info\n",
      "\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m docs_with_content = \u001b[43mextract_article_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Print detailed results\u001b[39;00m\n",
      "\u001b[32m    213\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtracted content details:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[148]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mextract_article_content\u001b[39m\u001b[34m(docs, api_key, output_dir)\u001b[39m\n",
      "\u001b[32m    181\u001b[39m         stats[\u001b[33m\"\u001b[39m\u001b[33mnot_available\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n",
      "\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Rate limiting\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    186\u001b[39m \u001b[38;5;66;03m# Print final summary\u001b[39;00m\n",
      "\u001b[32m    187\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtraction Summary:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import urllib.parse\n",
    "from bs4 import BeautifulSoup\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def extract_article_content(docs, api_key, output_dir=\"article_content\"):\n",
    "    \"\"\"\n",
    "    Extract available article content, focusing on what's actually accessible\n",
    "    through the API rather than attempting to get complete full text\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    text_output_dir = os.path.join(output_dir, \"extracted_text\")\n",
    "    if not os.path.exists(text_output_dir):\n",
    "        os.makedirs(text_output_dir)\n",
    "\n",
    "    stats = {\n",
    "        \"total\": len(docs),\n",
    "        \"abstract_only\": 0,\n",
    "        \"partial_text\": 0,\n",
    "        \"full_text\": 0,\n",
    "        \"not_available\": 0\n",
    "    }\n",
    "\n",
    "    print(f\"Extracting content for {len(docs)} articles...\")\n",
    "    \n",
    "    # Process each document with a DOI\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\nProcessing document {i+1}/{len(docs)}...\")\n",
    "        \n",
    "        # Skip documents without DOI\n",
    "        if not doc.get(\"doi\"):\n",
    "            print(\"No DOI available, skipping\")\n",
    "            stats[\"not_available\"] += 1\n",
    "            continue\n",
    "            \n",
    "        doi = doc[\"doi\"]\n",
    "        title = doc[\"title\"]\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"DOI: {doi}\")\n",
    "        \n",
    "        # Generate a safe filename from title\n",
    "        safe_title = re.sub(r\"[^\\w\\s-]\", \"\", title)\n",
    "        safe_title = re.sub(r\"\\s+\", \"_\", safe_title)\n",
    "        safe_title = safe_title[:50]\n",
    "        year = doc.get(\"publication_date\", \"\")\n",
    "        \n",
    "        # Create object to store extracted content\n",
    "        content = {\n",
    "            \"doi\": doi,\n",
    "            \"title\": title,\n",
    "            \"year\": year,\n",
    "            \"abstract\": doc.get(\"abstract\", \"\"),\n",
    "            \"sections\": [],\n",
    "            \"full_text\": \"\",\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        # Try to get XML representation first (most structured)\n",
    "        encoded_doi = urllib.parse.quote(doi, safe=\"\")\n",
    "        url = f\"https://api.elsevier.com/content/article/doi/{encoded_doi}\"\n",
    "        \n",
    "        headers = {\n",
    "            \"X-ELS-APIKey\": api_key,\n",
    "            \"Accept\": \"text/xml\"\n",
    "        }\n",
    "        \n",
    "        # Set retrieved content flag\n",
    "        content_retrieved = False\n",
    "        \n",
    "        try:\n",
    "            print(\"Requesting XML content...\")\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                # Save the raw XML for reference\n",
    "                xml_filepath = os.path.join(output_dir, f\"{safe_title}_{year}.xml\")\n",
    "                with open(xml_filepath, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "                \n",
    "                # Parse the XML to extract content\n",
    "                try:\n",
    "                    # Register namespaces for XML parsing\n",
    "                    ns = {\n",
    "                        'dc': 'http://purl.org/dc/elements/1.1/',\n",
    "                        'prism': 'http://prismstandard.org/namespaces/basic/2.0/',\n",
    "                        'ja': 'http://www.elsevier.com/xml/ja/dtd',\n",
    "                        'ce': 'http://www.elsevier.com/xml/common/dtd',\n",
    "                        'xocs': 'http://www.elsevier.com/xml/xocs/dtd'\n",
    "                    }\n",
    "                    \n",
    "                    root = ET.fromstring(response.content)\n",
    "                    \n",
    "                    # Extract the abstract if available\n",
    "                    abstract_elem = root.find('.//dc:description', ns)\n",
    "                    if abstract_elem is not None and abstract_elem.text:\n",
    "                        content[\"abstract\"] = abstract_elem.text\n",
    "                        \n",
    "                    # Extract full text sections if available\n",
    "                    # Approach varies by document format and availability\n",
    "                    full_text_sections = []\n",
    "                    \n",
    "                    # Try to find sections in the full-text element\n",
    "                    for section_elem in root.findall('.//ce:sections/ce:section', ns) + root.findall('.//section', ns):\n",
    "                        section_title_elem = section_elem.find('./ce:section-title', ns) or section_elem.find('./section-title', ns)\n",
    "                        section_title = section_title_elem.text if section_title_elem is not None else \"Unnamed Section\"\n",
    "                        \n",
    "                        # Collect all paragraph texts\n",
    "                        paragraphs = []\n",
    "                        for p_elem in section_elem.findall('.//ce:para', ns) + section_elem.findall('.//p', ns):\n",
    "                            if p_elem.text:\n",
    "                                paragraphs.append(p_elem.text)\n",
    "                        \n",
    "                        if paragraphs:\n",
    "                            section_text = \"\\n\\n\".join(paragraphs)\n",
    "                            full_text_sections.append({\n",
    "                                \"title\": section_title,\n",
    "                                \"text\": section_text\n",
    "                            })\n",
    "                    \n",
    "                    # If we found structured sections\n",
    "                    if full_text_sections:\n",
    "                        content[\"sections\"] = full_text_sections\n",
    "                        content_retrieved = True\n",
    "                        \n",
    "                        # Combine all sections into full text\n",
    "                        full_text_parts = []\n",
    "                        for section in full_text_sections:\n",
    "                            full_text_parts.append(f\"## {section['title']}\\n\\n{section['text']}\")\n",
    "                        \n",
    "                        content[\"full_text\"] = \"\\n\\n\".join(full_text_parts)\n",
    "                        \n",
    "                        # Determine content completeness level\n",
    "                        total_text_length = len(content[\"full_text\"])\n",
    "                        if total_text_length > 5000:  # Arbitrary threshold\n",
    "                            stats[\"full_text\"] += 1\n",
    "                        else:\n",
    "                            stats[\"partial_text\"] += 1\n",
    "                    \n",
    "                    # If we only got the abstract\n",
    "                    elif content[\"abstract\"]:\n",
    "                        content_retrieved = True\n",
    "                        stats[\"abstract_only\"] += 1\n",
    "                    else:\n",
    "                        # Try alternative parsing with BeautifulSoup\n",
    "                        soup = BeautifulSoup(response.content, 'xml')\n",
    "                        \n",
    "                        # Extract abstract\n",
    "                        abstract = soup.find('abstract') or soup.find('dc:description')\n",
    "                        if abstract and abstract.text:\n",
    "                            content[\"abstract\"] = abstract.text.strip()\n",
    "                            content_retrieved = True\n",
    "                            stats[\"abstract_only\"] += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing XML: {str(e)}\")\n",
    "                \n",
    "                # Save the extracted content as JSON\n",
    "                text_filepath = os.path.join(text_output_dir, f\"{safe_title}_{year}.json\")\n",
    "                with open(text_filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(content, f, indent=2, ensure_ascii=False)\n",
    "                \n",
    "                print(f\"Content saved to {text_filepath}\")\n",
    "                doc[\"content_file\"] = text_filepath\n",
    "                doc[\"xml_file\"] = xml_filepath\n",
    "            \n",
    "            else:\n",
    "                print(f\"Failed to retrieve XML content: {response.status_code}\")\n",
    "                print(f\"Error response: {response.text[:200]}...\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving XML: {str(e)}\")\n",
    "        \n",
    "        # If nothing was retrieved, increment counter\n",
    "        if not content_retrieved:\n",
    "            stats[\"not_available\"] += 1\n",
    "            \n",
    "        # Rate limiting\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Print final summary\n",
    "    print(\"\\nExtraction Summary:\")\n",
    "    print(f\"Total documents processed: {stats['total']}\")\n",
    "    print(f\"Full text extracted: {stats['full_text']} ({stats['full_text']*100/stats['total']:.1f}%)\")\n",
    "    print(f\"Partial text extracted: {stats['partial_text']} ({stats['partial_text']*100/stats['total']:.1f}%)\")\n",
    "    print(f\"Abstract only: {stats['abstract_only']} ({stats['abstract_only']*100/stats['total']:.1f}%)\")\n",
    "    print(f\"No content available: {stats['not_available']} ({stats['not_available']*100/stats['total']:.1f}%)\")\n",
    "    \n",
    "    return docs\n",
    "\n",
    "# Install BeautifulSoup if not already installed\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install beautifulsoup4\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "# Call the function with your filtered docs\n",
    "output_dir = \"article_content\"\n",
    "api_key = client.api_key\n",
    "\n",
    "# For testing, just use a few documents first\n",
    "test_docs = doc_info\n",
    "docs_with_content = extract_article_content(test_docs, api_key, output_dir)\n",
    "\n",
    "# Print detailed results\n",
    "print(\"\\nExtracted content details:\")\n",
    "for doc in docs_with_content:\n",
    "    print(f\"\\nTitle: {doc['title']}\")\n",
    "    print(f\"DOI: {doc.get('doi', 'N/A')}\")\n",
    "    \n",
    "    # Show what files are available\n",
    "    if \"content_file\" in doc:\n",
    "        print(f\"Content file: {doc['content_file']}\")\n",
    "        \n",
    "        # Try to load the content to show sections\n",
    "        try:\n",
    "            with open(doc['content_file'], 'r', encoding='utf-8') as f:\n",
    "                content = json.load(f)\n",
    "                \n",
    "            print(f\"Abstract available: {'Yes' if content.get('abstract') else 'No'}\")\n",
    "            print(f\"Number of extracted sections: {len(content.get('sections', []))}\")\n",
    "            \n",
    "            # Show section titles\n",
    "            if content.get('sections'):\n",
    "                print(\"Extracted sections:\")\n",
    "                for i, section in enumerate(content['sections']):\n",
    "                    print(f\"- {section['title']} ({len(section['text'])} characters)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading content file: {str(e)}\")\n",
    "    else:\n",
    "        print(\"No content was extracted\")\n",
    "\n",
    "# Save the results to a JSON file for future reference\n",
    "with open(os.path.join(output_dir, \"document_metadata.json\"), \"w\") as f:\n",
    "    json.dump(docs_with_content, f, indent=2)\n",
    "\n",
    "print(f\"\\nDocument metadata saved to {os.path.join(output_dir, 'document_metadata.json')}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
