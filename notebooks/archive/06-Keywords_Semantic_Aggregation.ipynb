{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Normalized Keyword  Count\n",
      "439                          smart city     56\n",
      "210      generative adversarial network     31\n",
      "138                       deep learning     26\n",
      "209                       generative ai     20\n",
      "295                large language model     18\n",
      "198                                 gan     17\n",
      "211  generative adversarial network gan     17\n",
      "23              artificial intelligence     16\n",
      "310                    machine learning     11\n",
      "509                     urban computing     10\n",
      "540             variational autoencoder     10\n",
      "290                                lstm      9\n",
      "274                   internet of thing      9\n",
      "253                                 iot      8\n",
      "149                     diffusion model      8\n",
      "513                      urban planning      7\n",
      "400                      remote sensing      7\n",
      "151                        digital twin      6\n",
      "125                   data augmentation      6\n",
      "49                   autonomous driving      6\n",
      "39                    anomaly detection      6\n",
      "539         variational autoencoder vae      6\n",
      "215                   generative design      5\n",
      "187                  federated learning      5\n",
      "11                                   ai      5\n",
      "46                  attention mechanism      5\n",
      "861                                          5\n",
      "214  generative artificial intelligence      4\n",
      "35                            algorithm      4\n",
      "275               internet of thing iot      4\n",
      "280          intrusion detection system      4\n",
      "289                                 llm      4\n",
      "296            large language model llm      4\n",
      "337                      neural network      4\n",
      "50                   autonomous vehicle      4\n",
      "472                      synthetic data      4\n",
      "122                         data mining      3\n",
      "516                   urban development      3\n",
      "586                         autoencoder      3\n",
      "602                             chatbot      3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joaocarlos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/joaocarlos/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joaocarlos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/joaocarlos/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "# Load JSON data\n",
    "with open(\"../data/04_author_keywords.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Normalization function\n",
    "def normalize_keyword(keyword):\n",
    "    keyword = keyword.lower()\n",
    "    keyword = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", keyword)\n",
    "    keyword = re.sub(r\"\\s+\", \" \", keyword).strip()\n",
    "    tokens = word_tokenize(keyword)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "# Aggregate keywords\n",
    "aggregated_keywords = defaultdict(int)\n",
    "\n",
    "for keyword, count in data.items():\n",
    "    normalized = normalize_keyword(keyword)\n",
    "    aggregated_keywords[normalized] += count\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_aggregated = pd.DataFrame(\n",
    "    aggregated_keywords.items(), columns=[\"Normalized Keyword\", \"Count\"]\n",
    ")\n",
    "df_aggregated.sort_values(by=\"Count\", ascending=False, inplace=True)\n",
    "\n",
    "# Display results\n",
    "print(df_aggregated.head(40))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
